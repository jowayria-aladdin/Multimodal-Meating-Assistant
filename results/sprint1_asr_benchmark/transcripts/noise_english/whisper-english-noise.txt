 Hey, Adam, let's quickly finalize what we will do in Sprint 1. Sure. So our goal is to benchmark existing transcription tools like Whisper, Google Speech-to-Text, and Otter.ai. Right. We need to collect a few short meeting recordings, English, Arabic, and a mixed one with some code switching. Exactly. Then we will run each OD file through those systems and compare the transcripts. For metrics, we will focus on word error rate and also check how each one handles noise and speaker separation. Yeah, especially in noisy or mixed language recordings. That's where Whisper usually does better. I will handle testing on Google Speech-to-Text and Otter. Can you run Whisper locally? Sure thing. I already set up the model in my machine. Nice. Then we will prepare a short report like 2 or 3 pages, summarizing the results and then we will run it on the machine. Okay. So we will run the report on the machine. Results and limitations. Perfect. Let's aim to finish this by the weekend, so we can move to sign language handling next Sprint. Deal. I will start by gathering the test audios today. Thank you, Ziyad. You're welcome, Adam.
