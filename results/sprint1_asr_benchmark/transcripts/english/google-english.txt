Hey Adam, let's quickly finalize what we will do in Sprint 1. Sure. So our goal is to benchmark existing transcription tools like Whisper, Google Speech to Text, and Auto AI.Right, we need to collect a few short meeting recordings, English, Arabic and a mixed one with some code switching exactly. Then we will run each of the files through those systems and compare the transcripts.For metrics, we will focus on word error rate and also check how each one handles noise and speaker separation. Yeah, especially in noisy or mixed language recordings. That's where Whisper usually does better.I will handle testing on Google speech to text and other. Can you run whisper locally? Sure thing, I already set up the model on my machine.Nice. Then we will prepare a short report, like two or three pages summarizing the results and limitations. Perfect. Let's aim to finish this by the weekend so we can move to sign language handling next Sprint.Then I'm start by gathering the test or use today. Thank you yet.